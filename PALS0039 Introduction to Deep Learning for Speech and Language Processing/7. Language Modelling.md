> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [www.phon.ucl.ac.uk](https://www.phon.ucl.ac.uk/courses/pals0039/week7.php)

Week 7 - Language Modelling
---------------------------

In which we build models that predict the continuation of sentences and which can be used to generate text and improve the accuracy of speech recognition and machine translation.

### Learning Objectives

By the end of the session the student will be able to:

*   explain the operation of language models
*   describe how language models can be built from corpora using n-gram statistics
*   describe some of the problems in using n-grams for language modelling
*   explain how the performance of language models can be computed using perplexity
*   describe how language models can be built using recurrent neural networks
*   explain how language can be generated from language models
*   describe some applications of language models
*   use Keras to implement, train and evaluate a language model

### Outline

1.  Statistical language models
2.  Estimating probabilities of sentences
3.  Using n-grams to create a language model
4.  Need for smoothing in n-gram language models
5.  Building recurrent neural language models
6.  Using LM for language generation
7.  Use of language models in machine translation
8.  Use of language models in speech recognition

### Research Paper of the Week

*   D. Kauchak, [Improving Text Simplification Language Modeling Using Unsimplified Text Data](https://www.aclweb.org/anthology/P13-1151.pdf) Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, 2013.

### Web Resources

*   [Talk to Transformer](https://talktotransformer.com/). See how a state-of-the-art language model can generate text given a starting prompt.

### Readings

Be sure to read one or more of these discussions of language models:

*   [N-gram Language models](https://web.stanford.edu/~jurafsky/slp3/3.pdf) from Spoken Language processing (3rd edition), Jurafsky & Martin, 2019.
*   [Sequence Processing with Recurrent Networks](https://web.stanford.edu/~jurafsky/slp3/9.pdf) from Spoken Language processing (3rd edition), Jurafsky & Martin, 2019.
*   [Probability for linguists](http://people.cs.uchicago.edu/~jagoldsm/Papers/probability.pdf). An introduction to probability theory for Linguists.
*   [Rethinking language: How probabilities shape the words we use](https://www.pnas.org/content/pnas/108/10/3825.full.pdf) by Thomas Griffiths.

### Exercises

1.  [Character level language model](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Exercise_7_1.ipynb) [[Answers](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_7_1.ipynb)]
2.  [Cloze task predictor using word language model](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Exercise_7_2.ipynb) [[Answers](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_7_2.ipynb)]

<ol> <li>Train a character-based language model (https://machinelearningmastery.com/develop-character-based-neural-language-model-keras/) ELMO: https://www.youtube.com/watch?v=DXdJlY35eVE http://hunterheidenreich.com/blog/elmo-word-vectors-in-keras/ <li>Parsing with SyntaxNet and Parsey McParseface (https://github.com/tensorflow/models/tree/master/research/syntaxnet) <li>Train number synthesis system (using World vocoder and MERLIN?) </ol>

Word count: 351. Last modified: 10:36 28-Feb-2020.