> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [www.phon.ucl.ac.uk](https://www.phon.ucl.ac.uk/courses/pals0039/week8.php)

Week 8 - Sequence to Sequence Models
------------------------------------

In which we discuss networks that transform between sequences, with particular interest in machine translation.

### Learning Objectives

By the end of the session the student will be able to:

*   outline the history of machine translation
*   describe metrics used for evaluation of machine translation systems
*   explain the operation of sequence-to-sequence models
*   discuss how limitations of the simple seqseq model is overcome through the use of attention
*   outline how limitations of the recurrent seq2seq model can be addressed using the transformer architecture
*   describe the application of sequence-to-sequence models to machine translation
*   use Keras to implement and train a sequence-to-sequence model

### Outline

1.  History of machine translation
2.  Metrics for evaluating performance of machine translation
3.  Sequence to sequence models
4.  Attention models
5.  Transformer architecture
6.  Multilingual machine translation
7.  Other applications of seq2seq models

### Research Paper of the Week

*   L. Dong, M. Lapata, [Language to Logical Form with Neural Attention](https://www.aclweb.org/anthology/P16-1004.pdf), Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 2016.

### Web Resources

*   [The illustrated seq2seq with attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/). A visual guide to how attention works in seq2seq models.
*   [The illustrated transformer](http://jalammar.github.io/illustrated-transformer/). A visual guide to how transformers work.
*   [How to Make a Language Translator](https://www.youtube.com/watch?v=nRBnh4qbPHI). Lively video.
*   [OpenNMT: An open source neural machine translation system](https://opennmt.net/).

### Readings

Be sure to read one or more of these discussions of sequence-to-sequence models:

*   [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf), Sutskever et al, 2014.
*   [Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144)
*   [Creating A Language Translation Model Using Sequence To Sequence Learning Approach](https://chunml.github.io/ChunML.github.io/project/Sequence-To-Sequence/)
*   [A history of machine translation from the Cold War to deep learning](https://www.freecodecamp.org/news/a-history-of-machine-translation-from-the-cold-war-to-deep-learning-f1d335ce8b5/)
*   [The Shallowness of Google Translate](https://www.theatlantic.com/technology/archive/2018/01/the-shallowness-of-google-translate/551570/) by Douglas Hofstadter.

### Exercises

1.  [Grapheme to phoneme conversion](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Exercise_8_1.ipynb) [[Answers](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_8_1.ipynb)]
2.  [Sentence embedding encoder](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Exercise_8_2.ipynb) [[Answers](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_8_2.ipynb)]
3.  [Machine translation](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Exercise_8_3.ipynb) [[Answers](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_8_3.ipynb)]

<ol> <li>Character to character sequence-to-sequence: https://keras.io/examples/lstm_seq2seq/ <li>Letter-to-sound? https://arxiv.org/abs/1506.00196 https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43264.pdf https://www.mdpi.com/2076-3417/9/6/1143/htm <li>Train simple machine translation (Ganegedera Ch10) <li>Generating text from Word2Vec (Ganegedera Ch8) <li>Use existing image classification model as front end to text annotation (Ganegedera Ch9) </ol>

Word count: 349. Last modified: 16:40 05-Mar-2020.