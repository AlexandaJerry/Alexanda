> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [www.phon.ucl.ac.uk](https://www.phon.ucl.ac.uk/courses/pals0039/week5.php)

Week 5 - Lexical Semantics and Word Embeddings
----------------------------------------------

In which we investigate vector representations of word semantics, methods for computing word similarity and the Word2Vec method for unsupervised learning of word embeddings.

### Learning Objectives

By the end of the session the student will be able to:

*   explain the advantages in representing the meaning of words and sentences as feature vectors
*   explain how vector representations of word meaning arise from the distributional hypothesis
*   outline the Word2Vec method for unsupervised learning of word embeddings
*   describe some applications of word embeddings
*   use the Keras toolkit to train word embeddings and use them for text processing

### Outline

1.  Lexical semantics: terminology and word relations
2.  Vector semantics: arising from the distributional hypothesis
3.  Deriving semantic vectors from collocations
4.  Computing word similarity from word vectors
5.  Presenting the Word2Vec method for learning word embeddings
6.  Applications of word embeddings
7.  Other approaches to calculating word embeddings
8.  Using pre-calculated word embeddings with Keras

### Research Paper of the Week

*   B. Eisner, T. Rocktäschel, I. Augenstein, M. Bošnjak, S. Reidel, [emoji2vec: Learning Emoji Representations from their Description](https://www.aclweb.org/anthology/W16-6208.pdf), Proceedings of the 4th International Workshop on Natural Language Processing for Social Media, EMNLP 2016.

### Web Resources

[Word Vector Representations: word2vec](https://www.youtube.com/watch?v=ERibwqs9p38). Stanford University video course: Natural Language Processing with Deep Learning.

### Readings

Be sure to read one or more of these discussions of word embedding:

*   [Vector Semantics and Embeddings](https://web.stanford.edu/~jurafsky/slp3/6.pdf)from Spoken Language processing (3rd edition), Jurafsky & Martin, 2019.
*   [A beginners guide to Word2Vec and Neural Word Embeddings](https://skymind.ai/wiki/word2vec).
*   [Word2Vec tutorial: the skip-gram model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
*   [Word2Vec in Keras tutorial](https://adventuresinmachinelearning.com/word2vec-keras-tutorial/)

### Exercises

1.  [Word similarity in Wordnet](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Exercise_5_1.ipynb) [[Answers](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_5_1.ipynb)]
2.  [Training Word Embeddings](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Exercise_5_2.ipynb) [[Answers](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_5_2.ipynb)]
3.  [Using Word Embeddings](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Exercise_5_3.ipynb) [[Answers](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_5_3.ipynb)]

<ol> <li>Using WordNet for representation (Ganegedera Ch3) <li>Learn a simple embedding from text (Ganegedera Ch3) <li>Study synonyms in existing embedding (e.g. Glove, Keras p190) <li>Use word embeddings for Reuters document classification (Ganegedera 4 & Keras 3.5, dataset built-in to Keras) </ol>

Word count: 331. Last modified: 14:10 04-Feb-2020.