> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [www.phon.ucl.ac.uk](https://www.phon.ucl.ac.uk/courses/pals0039/week3.php)

Week 3 - Artificial Neural Networks
-----------------------------------

In which we look at the general structure of artificial neural networks, including the mathematical description of processing within a node, and how networks can learn by gradient descent.

### Learning Objectives

By the end of the session the student will be able to:

*   describe the key developments in the history of artificial neural networks for machine learning.
*   describe the perceptron learning algorithm
*   explain how gradient descent works in multiple-layer networks
*   use the Keras toolkit to implement, train and test neural network models for simple problems

### Outline

#### *   Neural networks for machine learning

We view the brain as an information processing system, looking at the operation of neurons and how a network of neurons can perform complex calculations. We discuss how artificial neural networks can be motivated by the processing networks of the brain without being simulations of biological neurons.

*   R. Nagyfi, [The differences between artificial and biological neural networks](https://towardsdatascience.com/the-differences-between-artificial-and-biological-neural-networks-a8b46db828b7).

#### *   The Perceptron

We discuss a simple mathematical of a neuron proposed by McCulloch and Pitts, and a means to train such a model from data proposed by Rosenblatt: the Perceptron learning rule.

*   Warren McCulloch and Walter Pitts, [A Logical Calculus of Ideas Immanent in Nervous Activity](https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf), 1943
*   Frank Rosenblatt, [The Perceptron — a perceiving and recognizing automaton](https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf). 1957
*   [Perceptron learning animation](https://www.youtube.com/watch?v=d3J9FbwkG24)

#### *   Multiple layers of perceptrons

We look at the criticisms of perceptrons as a means to perform information processing, and a solution to the problem of training multiple layers of perceptrons through the use of gradient descent.

*   Marvin Minsky and Seymour Papert, [Perceptrons](https://mitpress.mit.edu/books/perceptrons), 1969
*   [Introduction to learning by gradient descent](https://builtin.com/data-science/gradient-descent)
*   [Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)

#### *   Deep neural networks

We briefly review the history of how networks of multi-layer perceptrons turned into "deep" networks. Problems in extending gradient descent to large networks were gradually overcome by improvements in algorithms and an increase in computer power. We outline common activation functions, loss functions and optimisation methods.

*   A. Krizhevky, I. Sutskever, G. Hinton, [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf), NeurIPS, 2012.
*   D. Kingma, J. Ba, [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980), 2015.

#### *   Programing DNNs

We introduce the Keras toolkit for building, training and running deep neural networks.

*   [Getting started with Keras](https://keras.io/getting_started/intro_to_keras_for_engineers/)
*   [Activation functions available in Keras](https://keras.io/api/layers/activations/)
*   [Loss functions available in Keras](https://keras.io/api/losses/)
*   [Optimisation functions avaialble in Keras](https://keras.io/api/optimizers/)

### Research Paper of the Week

*   I. Howard, M. Huckvale, [Two-level recognition of isolated words using neural nets](https://www.phon.ucl.ac.uk/courses/pals0039/papers/HowHuc89.pdf), First IEE International Conference on Artificial Neural Networks, 1989.

### Web Resources

*   [How Deep Neural Networks Work](https://www.youtube.com/watch?v=ILsA4nyG7I0), excellent introductory video. [If you want to be challenged, try to build a version of this example in Keras - classify 4-pixel inputs as solid, or made up of a vertical, horizontal or diagonal line]
*   LinkedIn Video Course: **Building recommender systems with machine learning and AI: History of artificial neural networks**. Accessible to all UCL staff and students through [this sign on](https://www.linkedin.com/checkpoint/enterprise/login/69919578?application=learning).
*   LinkedIn Video Course: **Artificial intelligence foundations neural networks**. Accessible to all UCL staff and students through [this sign on](https://www.linkedin.com/checkpoint/enterprise/login/69919578?application=learning).
*   [Introduction to Neural Networks](https://www.3blue1brown.com/neural-networks) YouTube Videos by 3Blue1Brown.

### Readings

Be sure to read one or more of these discussions of deep learning:

*   [A brief history of neural nets and deep learning](https://www.skynettoday.com/overviews/neural-net-history/).
*   [Keras tutorial: deep learning in Python](https://elitedatascience.com/keras-tutorial-deep-learning-in-python).
*   [Your First Deep Learning Project in Python with Keras Step-By-Step](https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/).

### Exercises

1.  [Gradient Descent](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Exercise_3_1.ipynb) [[Answers](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_3_1.ipynb)]
2.  [Autoencoder](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Exercise_3_2.ipynb) [[Answers](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_3_2.ipynb)]
3.  [Vowel Classifier](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Exercise_3_3.ipynb) [[Answers](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_3_3.ipynb)]

Word count: 599. Last modified: 14:29 21-Jan-2021.