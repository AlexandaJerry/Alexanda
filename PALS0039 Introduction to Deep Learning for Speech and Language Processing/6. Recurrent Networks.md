> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [www.phon.ucl.ac.uk](https://www.phon.ucl.ac.uk/courses/pals0039/week6.php)

Week 6 - Recurrent Networks
---------------------------

In which we discuss the construction of networks that can process variable length sequences by maintaining a memory of past input, and demonstrate how they can be applied to spoken and written utterances.

### Learning Objectives

By the end of the session the student will be able to:

*   describe the need for systems with memory when processing sequences
*   describe the structure and operation of simple recurrent networks
*   implement a recurrent network for a simple problem in classifying sequences
*   understand the limitations of simple recurrent layers and training
*   describe some more advanced types of recurrent nodes
*   use Keras to implement and train recurrent networks to solve sequence classification problems in both speech and text

### Outline

1.  Treating text and speech as sequences with contextual dependency
2.  The need for memory in sequence processing
3.  Structure of Recurrent Neural Networks (RNN)
4.  Training of RNN by unrolling over the sequence
5.  Application of RNN to signal processing
6.  Advanced RNN nodes: Long Short-Term Memory network (LSTM) and Gated Recurrent Units (GRU)
7.  Application of LSTM to sequence classification
8.  Contextualised word embeddings with bidirectional LSTMs

### Research Paper of the Week

*   S. Karlekar, T. Niu, M. Bansal, [Detecting Linguistic Characteristics of Alzheimer’s Dementia by Interpreting Neural Models](https://www.aclweb.org/anthology/N18-2110.pdf), Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2018.

### Web Resources

*   LinkedIn Video Course: **Building recommender systems with machine learning and ai**, in particular: **Intro to recurrent neural networks**. Accessible to all UCL staff and students through [this sign on](https://www.linkedin.com/checkpoint/enterprise/login/69919578?application=learning).

### Readings

Be sure to read one or more of these discussions of recurrent neural networks:

*   [Understanding RNNs and LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
*   [Introduction to recurrent neural networks](https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/) (With calculations).
*   [Introduction to recurrent neural networks](https://www.jeremyjordan.me/introduction-to-recurrent-neural-networks/)

### Exercises

1.  [Sentiment analysis with RNN and word embeddings](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Exercise_6_1.ipynb) [[Answers](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_6_1.ipynb)]
2.  [Command word speech recognizer](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Exercise_6_2.ipynb) [[Answers](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_6_2.ipynb)]
3.  [Phone posteriors from speech signal](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Exercise_6_3.ipynb) [[Answers](https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_6_3.ipynb)]

Word count: 361. Last modified: 14:33 19-Feb-2020.